{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalacion de librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\estiv\\anaconda3\\lib\\site-packages (3.8.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from spacy) (4.66.5)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from spacy) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from spacy) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost==2.1.4 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from xgboost==2.1.4) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from xgboost==2.1.4) (1.13.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost==2.1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import joblib\n",
    "import unidecode\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "except:\n",
    "    # Si no está instalado, instalarlo\n",
    "    print(\"Instalando modelo de spaCy para español...\")\n",
    "    !python -m spacy download es_core_news_sm\n",
    "    nlp = spacy.load(\"es_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creacion de transformer personalizados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para automatizar todo el preprocesamiento de datos realizado en la etapa 1 del proyecto, utilizamos las clases BaseEstimator y TransformerMixin que nos permiten incorporar nuestro preprocesamiento manual como un paso dentro del pipeline. Esta estructura nos ofrece la ventaja de que, al exportar el pipeline y usar el método fit, cada función transformer se aplica automáticamente a los datos en secuencia.\n",
    "Además, incorporamos un transformer no personalizado para la vectorización: TfidfVectorizer. Este componente simplemente se importa y se integra en el pipeline, sin necesidad de programarlo desde cero. Esta combinación de transformers personalizados y predefinidos nos permite crear un flujo de trabajo eficiente que procesa los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Transformer para tokenizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTokenizer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, text_col='Descripcion'):\n",
    "        self.text_col = text_col\n",
    "        try:\n",
    "            word_tokenize(\"Prueba\")\n",
    "        except:\n",
    "            import nltk\n",
    "            nltk.download('punkt')\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        df_copy = X.copy()\n",
    "        \n",
    "        df_copy['words'] = df_copy[self.text_col].apply(self._tokenize_text)\n",
    "        \n",
    "        return df_copy\n",
    "    \n",
    "    def _tokenize_text(self, text):\n",
    "        if not isinstance(text, str):\n",
    "            return []\n",
    "        \n",
    "        # Tokenizar el texto\n",
    "        tokens = word_tokenize(text)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al inicializarse, intenta tokenizar una palabra de prueba (\"Prueba\") y descarga el tokenizador 'punkt' de NLTK si es necesario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El transformador toma un DataFrame, crea una copia, y aplica una función de tokenización a la columna de texto especificada (por defecto 'Descripcion'). Esta función divide el texto en palabras individuales (tokens) utilizando word_tokenize de NLTK y devuelve el DataFrame con una nueva columna 'words' que contiene las listas de tokens para cada texto.ReintentarClaude puede cometer errores. Verifique las respuestas.\n",
    "Es basicamente la misma tokenizacion realizada en la etapa 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Transformer para preprocesar el texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este transformer convierte todo a minusculas, elimina los signos de puntuacion, elimina los caracteres no ascii y elimina las stopwords todo tal cual como en la etapa 1 pero lo hace en esta clase para poder usarlo en el pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self.stopwords = set(stopwords.words('spanish'))\n",
    "        except:\n",
    "            import nltk\n",
    "            nltk.download('stopwords')\n",
    "            self.stopwords = set(stopwords.words('spanish'))\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        df_copy = X.copy()\n",
    "        \n",
    "        # Aplicar el preprocesamiento a la columna 'words'\n",
    "        df_copy['words'] = df_copy['words'].apply(self._preprocess_tokens)\n",
    "        \n",
    "        return df_copy\n",
    "    \n",
    "    def _preprocess_tokens(self, tokens):\n",
    "        \"\"\"Preprocesa una lista de tokens\"\"\"\n",
    "        if not isinstance(tokens, list):\n",
    "            return []\n",
    "        \n",
    "        # Convertir a minúsculas\n",
    "        tokens = self._to_lowercase(tokens)\n",
    "        \n",
    "        # Eliminar puntuación\n",
    "        tokens = self._remove_punctuation(tokens)\n",
    "        \n",
    "        # Eliminar caracteres no ASCII\n",
    "        tokens = self._remove_non_ascii(tokens)\n",
    "        \n",
    "        # Eliminar stopwords\n",
    "        tokens = self._remove_stopwords(tokens)\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def _to_lowercase(self, words):\n",
    "        \"\"\"Convertir a minúsculas\"\"\"\n",
    "        return [word.lower() for word in words if word is not None]\n",
    "    \n",
    "    def _remove_punctuation(self, words):\n",
    "        \"\"\"Eliminar puntuación\"\"\"\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            if word is not None:\n",
    "                new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "                if new_word != '':\n",
    "                    new_words.append(new_word)\n",
    "        return new_words\n",
    "    \n",
    "    def _remove_non_ascii(self, words):\n",
    "        \"\"\"Eliminar caracteres no ASCII\"\"\"\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            if word is not None:\n",
    "                new_word = unidecode.unidecode(word)\n",
    "                new_words.append(new_word)\n",
    "        return new_words\n",
    "    \n",
    "    def _remove_stopwords(self, words):\n",
    "        \"\"\"Eliminar stopwords\"\"\"\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            if word not in self.stopwords:\n",
    "                new_words.append(word)\n",
    "        return new_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al inicializarse, carga las palabras vacías (stopwords) en español, descargándolas si es necesario. Cuando se ejecuta su transformación, toma los tokens previamente generados y les aplica una serie de operaciones de limpieza: convierte todo a minúsculas, elimina los signos de puntuación, remueve caracteres especiales no ASCII (convirtiéndolos a su equivalente sin acentos o caracteres especiales), y finalmente filtra las palabras vacías que no aportan significado al análisis. El resultado es una lista de tokens limpios y estandarizados que facilitan el análisis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Transformer para hacer lematizacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que en la etapa 1 se hace la misma lematizacion que lleva las palabras a su forma raiz para mejorar el modelo y que no cuente palabras conjugadas como si fueran otras palabras sino que sean todas la misma palabra raiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lemmatizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        # Asegurarnos de que el modelo de spaCy está cargado\n",
    "        try:\n",
    "            self.nlp = spacy.load(\"es_core_news_sm\")\n",
    "        except:\n",
    "            import sys\n",
    "            !python -m spacy download es_core_news_sm\n",
    "            self.nlp = spacy.load(\"es_core_news_sm\")\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Crear copia del DataFrame\n",
    "        df_copy = X.copy()\n",
    "        \n",
    "        # Aplicar lematización a la columna 'words'\n",
    "        df_copy['words'] = df_copy['words'].apply(self._lemmatize_tokens)\n",
    "        \n",
    "        return df_copy\n",
    "    \n",
    "    def _lemmatize_tokens(self, tokens):\n",
    "        \"\"\"Lematiza una lista de tokens\"\"\"\n",
    "        if not isinstance(tokens, list):\n",
    "            return []\n",
    "        \n",
    "        # Unir los tokens en una cadena \n",
    "        text = \" \".join(tokens)\n",
    "        \n",
    "        # Procesar el texto con spaCy\n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        # Lematizar, enfocándose en los verbos\n",
    "        lemmatized_tokens = [token.lemma_ if token.pos_ == \"VERB\" else token.text \n",
    "                            for token in doc]\n",
    "        \n",
    "        return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al inicializarse, carga el modelo de lenguaje español de spaCy (\"es_core_news_sm\"), descargándolo si no está disponible. Durante la transformación, toma las listas de tokens previamente procesados, los une en un texto, procesa este texto con el modelo de spaCy y extrae los lemas (formas base) de los verbos, mientras mantiene el resto de las palabras sin cambios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Transformer para unir los tokens   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une los tokens para poder hacer vectoriazacion prosteriormente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JoinTokens(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, words_col='words'):\n",
    "        self.words_col = words_col\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Obtenemos solo la columna de tokens\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            # Si es un DataFrame, extraemos solo la columna de tokens\n",
    "            tokens_lists = X[self.words_col]\n",
    "        else:\n",
    "            # Si ya es una serie o lista\n",
    "            tokens_lists = X\n",
    "            \n",
    "        # Unimos los tokens en cadenas de texto\n",
    "        return [' '.join(tokens) if isinstance(tokens, list) else '' for tokens in tokens_lists]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta clase JoinTokens se encarga de convertir las listas de tokens nuevamente en cadenas de texto. Al transformar los datos, toma el DataFrame de entrada y verifica si es efectivamente un DataFrame para extraer solo la columna de tokens, o si ya es una serie o lista para procesarla directamente. Luego une cada lista de tokens en una sola cadena de texto utilizando espacios como separadores. El resultado es una lista de textos procesados donde cada elemento corresponde a un documento original, pero ahora con sus palabras unidas después de haber pasado por todo el proceso de limpieza y normalización. Este paso es esencial para devolver los tokens a un formato de texto que pueda ser utilizado por el vectorizador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Creacion del pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el pipeline y en sus pasos agregamos de forma ordenada los transformers definidos previamente, por ultimo el modelo XGBoost que fue el mejor modelo de nuestra etapa 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_classification_pipeline():\n",
    "    pipeline = Pipeline([\n",
    "        # 1. Tokenizar texto\n",
    "        ('tokenizer', TextTokenizer(text_col='Descripcion')),\n",
    "        \n",
    "        # 2. Preprocesar tokens\n",
    "        ('token_preprocessor', TokenPreprocessor()),\n",
    "        \n",
    "        # 3. Lematizar tokens\n",
    "        ('lemmatizer', Lemmatizer()),\n",
    "        \n",
    "        # 4. Unir tokens para vectorización\n",
    "        ('join_tokens', JoinTokens()),\n",
    "        \n",
    "        # 5. Vectorizar texto\n",
    "        ('vectorizer', TfidfVectorizer()),\n",
    "        \n",
    "        # 6. Clasificar\n",
    "        ('classifier', XGBClassifier(eval_metric='mlogloss',\n",
    "                                   n_estimators=100,\n",
    "                                   max_depth=5,\n",
    "                                   learning_rate=0.1,\n",
    "                                   random_state=42))\n",
    "    ])\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función crea un pipeline completo para la clasificación de textos. El pipeline comienza con el proceso de tokenización, luego limpia y preprocesa esos tokens, aplica lematización a los verbos, y une los tokens procesados en cadenas de texto que luego son vectorizadas mediante TF-IDF.\n",
    "Finalmente, utiliza XGBoost como clasificador con parámetros cuidadosamente seleccionados: 'eval_metric' está configurado como 'mlogloss' (pérdida logarítmica multiclase) para evaluar el rendimiento en problemas de clasificación múltiple; 'n_estimators=100' determina la cantidad de árboles en el bosque, equilibrando precisión y tiempo de entrenamiento; 'max_depth=5' limita la profundidad de los árboles para evitar sobreajuste; 'learning_rate=0.1' controla cuánto se ajusta el modelo con cada árbol, ofreciendo un buen balance entre velocidad de convergencia y precisión; y 'random_state=42' garantiza resultados reproducibles en cada ejecución.ReintentarClaude puede cometer errores. Verifique las respuestas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Entrenar, evaluar y exportar el modelo con su pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(df, text_col='Descripcion', target_col='Label', test_size=0.2, random_state=42):\n",
    "    # Eliminar duplicados y conflictos a nivel de dataframe\n",
    "    print(f\"Filas originales: {len(df)}\")\n",
    "    \n",
    "    # Detectar duplicados \n",
    "    duplicated_descriptions = df[df[text_col].duplicated(keep=False)]\n",
    "    \n",
    "    # Identificar duplicados conflictivos (mismo texto, diferente etiqueta)\n",
    "    conflict_groups = duplicated_descriptions.groupby(text_col)[target_col].nunique() > 1\n",
    "    conflict_texts = conflict_groups[conflict_groups].index\n",
    "    \n",
    "    # Eliminar filas con conflictos\n",
    "    conflict_rows = df[df[text_col].isin(conflict_texts)].index\n",
    "    df_clean = df.drop(index=conflict_rows)\n",
    "    print(f\"Filas eliminadas por conflictos: {len(conflict_rows)}\")\n",
    "    \n",
    "    # Eliminar duplicados restantes\n",
    "    df_clean = df_clean.drop_duplicates(subset=[text_col], keep='first')\n",
    "    print(f\"Filas después de eliminar duplicados y conflictos: {len(df_clean)}\")\n",
    "    \n",
    "    # Dividir en entrenamiento y prueba\n",
    "    df_train, df_test = train_test_split(\n",
    "        df_clean, test_size=test_size, random_state=random_state, stratify=df_clean[target_col]\n",
    "    )\n",
    "    \n",
    "    # Preparar X e y para entrenamiento\n",
    "    X_train = df_train[[text_col]].copy()\n",
    "    y_train = df_train[target_col]\n",
    "    \n",
    "    # Preparar X e y para prueba\n",
    "    X_test = df_test[[text_col]].copy()\n",
    "    y_test = df_test[target_col]\n",
    "    \n",
    "    # Crear y entrenar el pipeline\n",
    "    pipeline = create_text_classification_pipeline()\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluar el modelo\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    # Métricas\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score (weighted): {f1:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función se encarga de preparar los datos, entrenar el modelo y evaluar su rendimiento. Comienza con una preparación de los datos, para luego dividirlos en conjuntos de entrenamiento y prueba utilizando un tamaño de test especificado (por defecto 20%) y asegurando que la distribución de clases se mantenga proporcionalmente igual en ambos conjuntos mediante el parámetro 'stratify'.\n",
    "A continuación, prepara las variables predictoras (X) y objetivo (y) tanto para el entrenamiento como para la prueba. Crea el pipeline completo de clasificación de texto llamando a la función vista anteriormente y lo entrena con los datos preparados.\n",
    "Finalmente, evalúa el rendimiento del modelo generando predicciones sobre el conjunto de prueba y calculando varias métricas: precisión (accuracy), puntuación F1 ponderada y un informe de clasificación detallado que incluye precisión, recall y F1 para cada clase. La función retorna el pipeline entrenado, que puede ser utilizado posteriormente para hacer predicciones con nuevos datos o ser exportado para su uso en producción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_model(pipeline, file_path=\"modelo_clasificacion_texto.joblib\"):\n",
    "    joblib.dump(pipeline, file_path)\n",
    "    print(f\"Modelo exportado exitosamente a: {file_path}\")\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función se encarga de guardar el modelo de clasificación de texto entrenado para su uso posterior. Toma como entrada el pipeline completo que contiene todas las etapas de procesamiento y el modelo, y lo serializa utilizando la biblioteca joblib, guardándolo en un archivo en la ruta especificada (por defecto, \"modelo_clasificacion_texto.joblib\"). La serialización permite conservar todo el flujo de preprocesamiento y el modelo en un único archivo, facilitando su implementación en entornos de producción sin necesidad de reentrenar. La función confirma que el modelo ha sido exportado correctamente y devuelve la ruta del archivo donde se guardó."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"fake_news_spanish.csv\", sep=\";\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leemos el df de los datos originales y lo convertimos en df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Label</th>\n",
       "      <th>Titulo</th>\n",
       "      <th>Descripcion</th>\n",
       "      <th>Fecha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID</td>\n",
       "      <td>1</td>\n",
       "      <td>'The Guardian' va con Sánchez: 'Europa necesit...</td>\n",
       "      <td>El diario británico publicó este pasado jueves...</td>\n",
       "      <td>02/06/2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID</td>\n",
       "      <td>0</td>\n",
       "      <td>REVELAN QUE EL GOBIERNO NEGOCIO LA LIBERACIÓN ...</td>\n",
       "      <td>REVELAN QUE EL GOBIERNO NEGOCIO LA LIBERACIÓN ...</td>\n",
       "      <td>01/10/2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID</td>\n",
       "      <td>1</td>\n",
       "      <td>El 'Ahora o nunca' de Joan Fuster sobre el est...</td>\n",
       "      <td>El valencianismo convoca en Castelló su fiesta...</td>\n",
       "      <td>25/04/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID</td>\n",
       "      <td>1</td>\n",
       "      <td>Iglesias alienta a Yolanda Díaz, ERC y EH Bild...</td>\n",
       "      <td>En política, igual que hay que negociar con lo...</td>\n",
       "      <td>03/01/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID</td>\n",
       "      <td>0</td>\n",
       "      <td>Puigdemont: 'No sería ninguna tragedia una rep...</td>\n",
       "      <td>En una entrevista en El Punt Avui, el líder de...</td>\n",
       "      <td>09/03/2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Label                                             Titulo  \\\n",
       "0  ID      1  'The Guardian' va con Sánchez: 'Europa necesit...   \n",
       "1  ID      0  REVELAN QUE EL GOBIERNO NEGOCIO LA LIBERACIÓN ...   \n",
       "2  ID      1  El 'Ahora o nunca' de Joan Fuster sobre el est...   \n",
       "3  ID      1  Iglesias alienta a Yolanda Díaz, ERC y EH Bild...   \n",
       "4  ID      0  Puigdemont: 'No sería ninguna tragedia una rep...   \n",
       "\n",
       "                                         Descripcion       Fecha  \n",
       "0  El diario británico publicó este pasado jueves...  02/06/2023  \n",
       "1  REVELAN QUE EL GOBIERNO NEGOCIO LA LIBERACIÓN ...  01/10/2023  \n",
       "2  El valencianismo convoca en Castelló su fiesta...  25/04/2022  \n",
       "3  En política, igual que hay que negociar con lo...  03/01/2022  \n",
       "4  En una entrevista en El Punt Avui, el líder de...  09/03/2018  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas originales: 57063\n",
      "Filas eliminadas por conflictos: 13880\n",
      "Filas después de eliminar duplicados y conflictos: 42787\n",
      "Accuracy: 0.8675\n",
      "F1 Score (weighted): 0.8617\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.68      0.80      3386\n",
      "           1       0.83      0.99      0.90      5172\n",
      "\n",
      "    accuracy                           0.87      8558\n",
      "   macro avg       0.90      0.84      0.85      8558\n",
      "weighted avg       0.89      0.87      0.86      8558\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar el pipeline de entrenamiento y evaluación\n",
    "pipeline = train_and_evaluate(df, text_col='Descripcion', target_col='Label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutamos el pipeline entrenandolo y evaluandolo, las metricas son similares a las obtenidas en la etapa1, casi iguales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo exportado exitosamente a: ../model/modelo_clasificacion_texto.joblib\n"
     ]
    }
   ],
   "source": [
    "model_path = export_model(pipeline, \"../model/modelo_clasificacion_texto.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ultimo  exportamos el pipeline en la carpeta de la app para ser utilizado"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
