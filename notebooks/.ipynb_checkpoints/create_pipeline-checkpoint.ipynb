{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\estiv\\anaconda3\\lib\\site-packages (3.8.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from spacy) (4.66.5)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from spacy) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from spacy) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\estiv\\anaconda3\\lib\\site-packages (3.0.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\estiv\\anaconda3\\lib\\site-packages (from xgboost) (1.13.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import joblib\n",
    "import unidecode\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "except:\n",
    "    # Si no está instalado, instalarlo\n",
    "    print(\"Instalando modelo de spaCy para español...\")\n",
    "    !python -m spacy download es_core_news_sm\n",
    "    nlp = spacy.load(\"es_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTokenizer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, text_col='Descripcion'):\n",
    "        self.text_col = text_col\n",
    "        try:\n",
    "            word_tokenize(\"Prueba\")\n",
    "        except:\n",
    "            import nltk\n",
    "            nltk.download('punkt')\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        df_copy = X.copy()\n",
    "        \n",
    "        df_copy['words'] = df_copy[self.text_col].apply(self._tokenize_text)\n",
    "        \n",
    "        return df_copy\n",
    "    \n",
    "    def _tokenize_text(self, text):\n",
    "        if not isinstance(text, str):\n",
    "            return []\n",
    "        \n",
    "        # Tokenizar el texto\n",
    "        tokens = word_tokenize(text)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformador personalizado para preprocesar tokens ya existentes\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self.stopwords = set(stopwords.words('spanish'))\n",
    "        except:\n",
    "            import nltk\n",
    "            nltk.download('stopwords')\n",
    "            self.stopwords = set(stopwords.words('spanish'))\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Crear una copia del DataFrame para no modificar el original\n",
    "        df_copy = X.copy()\n",
    "        \n",
    "        # Aplicar el preprocesamiento a la columna 'words'\n",
    "        df_copy['words'] = df_copy['words'].apply(self._preprocess_tokens)\n",
    "        \n",
    "        return df_copy\n",
    "    \n",
    "    def _preprocess_tokens(self, tokens):\n",
    "        \"\"\"Preprocesa una lista de tokens\"\"\"\n",
    "        if not isinstance(tokens, list):\n",
    "            return []\n",
    "        \n",
    "        # Convertir a minúsculas\n",
    "        tokens = self._to_lowercase(tokens)\n",
    "        \n",
    "        # Eliminar puntuación\n",
    "        tokens = self._remove_punctuation(tokens)\n",
    "        \n",
    "        # Eliminar caracteres no ASCII\n",
    "        tokens = self._remove_non_ascii(tokens)\n",
    "        \n",
    "        # Eliminar stopwords\n",
    "        tokens = self._remove_stopwords(tokens)\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def _to_lowercase(self, words):\n",
    "        \"\"\"Convertir a minúsculas\"\"\"\n",
    "        return [word.lower() for word in words if word is not None]\n",
    "    \n",
    "    def _remove_punctuation(self, words):\n",
    "        \"\"\"Eliminar puntuación\"\"\"\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            if word is not None:\n",
    "                new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "                if new_word != '':\n",
    "                    new_words.append(new_word)\n",
    "        return new_words\n",
    "    \n",
    "    def _remove_non_ascii(self, words):\n",
    "        \"\"\"Eliminar caracteres no ASCII\"\"\"\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            if word is not None:\n",
    "                new_word = unidecode.unidecode(word)\n",
    "                new_words.append(new_word)\n",
    "        return new_words\n",
    "    \n",
    "    def _remove_stopwords(self, words):\n",
    "        \"\"\"Eliminar stopwords\"\"\"\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            if word not in self.stopwords:\n",
    "                new_words.append(word)\n",
    "        return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuplicateHandler(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformador que maneja duplicados para entrenamiento y es seguro para predicción\n",
    "    \"\"\"\n",
    "    def __init__(self, words_col='words', label_col='Label'):\n",
    "        self.words_col = words_col\n",
    "        self.label_col = label_col\n",
    "        self.keep_indices_ = None  # Almacenará los índices a mantener\n",
    "        self.is_fitted_ = False    # Para saber si estamos en modo entrenamiento o predicción\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # Trabajar con una copia para no modificar el original\n",
    "        df = X.copy()\n",
    "        \n",
    "        # Verificar si estamos en modo predicción (sin Label)\n",
    "        if self.label_col not in df.columns:\n",
    "            self.is_fitted_ = True\n",
    "            return self\n",
    "            \n",
    "        # Encontrar duplicados en la columna de palabras\n",
    "        duplicados = df[df[self.words_col].duplicated(keep=False)]\n",
    "        \n",
    "        # Convertir las listas a tuplas para poder usarlas como índices en groupby\n",
    "        duplicados_tuplas = duplicados[self.words_col].apply(tuple)\n",
    "        \n",
    "        # Encontrar los grupos donde hay conflicto en la etiqueta\n",
    "        grupos = duplicados.groupby(duplicados_tuplas)\n",
    "        conflictos = grupos[self.label_col].nunique() > 1\n",
    "        \n",
    "        # Obtener los índices de los duplicados conflictivos\n",
    "        indices_conflicto = conflictos[conflictos].index if len(conflictos) > 0 else []\n",
    "        filas_conflicto = duplicados[duplicados_tuplas.isin(indices_conflicto)].index if len(indices_conflicto) > 0 else []\n",
    "        \n",
    "        # Identificar las filas a mantener\n",
    "        df_temp = df.drop(index=filas_conflicto)\n",
    "        df_limpio = df_temp.drop_duplicates(subset=[self.words_col], keep=\"first\")\n",
    "        \n",
    "        # Guardar los índices de las filas que queremos mantener\n",
    "        self.keep_indices_ = df_limpio.index\n",
    "        \n",
    "        print(f\"Filas originales: {len(X)}\")\n",
    "        print(f\"Filas eliminadas por conflictos: {len(filas_conflicto)}\")\n",
    "        print(f\"Filas eliminadas por duplicados simples: {len(df_temp) - len(df_limpio)}\")\n",
    "        print(f\"Filas después de limpieza: {len(df_limpio)}\")\n",
    "        \n",
    "        self.is_fitted_ = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Si no estamos ajustados o no hay columna de palabras, devolver sin cambios\n",
    "        if not self.is_fitted_ or self.words_col not in X.columns:\n",
    "            return X\n",
    "            \n",
    "        # Si estamos en modo predicción (sin Label) o primer uso\n",
    "        if self.label_col not in X.columns:\n",
    "            # En predicción solo eliminamos duplicados, no buscamos conflictos\n",
    "            df_copy = X.copy()\n",
    "            return df_copy.drop_duplicates(subset=[self.words_col], keep=\"first\")\n",
    "            \n",
    "        # Si estamos en modo entrenamiento y tenemos los índices\n",
    "        if self.keep_indices_ is not None:\n",
    "            # Filtrar solo las filas que queríamos mantener\n",
    "            return X.loc[self.keep_indices_]\n",
    "            \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lemmatizer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformador personalizado para realizar la lematización de verbos\n",
    "    usando spaCy para una columna de tokens\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Asegurarnos de que el modelo de spaCy está cargado\n",
    "        try:\n",
    "            self.nlp = spacy.load(\"es_core_news_sm\")\n",
    "        except:\n",
    "            import sys\n",
    "            !python -m spacy download es_core_news_sm\n",
    "            self.nlp = spacy.load(\"es_core_news_sm\")\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Crear copia del DataFrame\n",
    "        df_copy = X.copy()\n",
    "        \n",
    "        # Aplicar lematización a la columna 'words'\n",
    "        df_copy['words'] = df_copy['words'].apply(self._lemmatize_tokens)\n",
    "        \n",
    "        return df_copy\n",
    "    \n",
    "    def _lemmatize_tokens(self, tokens):\n",
    "        \"\"\"Lematiza una lista de tokens\"\"\"\n",
    "        if not isinstance(tokens, list):\n",
    "            return []\n",
    "        \n",
    "        # Unir los tokens en una cadena para procesarlos con spaCy\n",
    "        text = \" \".join(tokens)\n",
    "        \n",
    "        # Procesar el texto con spaCy\n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        # Lematizar, enfocándose en los verbos\n",
    "        lemmatized_tokens = [token.lemma_ if token.pos_ == \"VERB\" else token.text \n",
    "                            for token in doc]\n",
    "        \n",
    "        return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JoinTokens(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformador para unir tokens en una cadena de texto\n",
    "    para preparar los datos para la vectorización\n",
    "    \"\"\"\n",
    "    def __init__(self, words_col='words'):\n",
    "        self.words_col = words_col\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Obtenemos solo la columna de tokens\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            # Si es un DataFrame, extraemos solo la columna de tokens\n",
    "            tokens_lists = X[self.words_col]\n",
    "        else:\n",
    "            # Si ya es una serie o lista\n",
    "            tokens_lists = X\n",
    "            \n",
    "        # Unimos los tokens en cadenas de texto\n",
    "        return [' '.join(tokens) if isinstance(tokens, list) else '' for tokens in tokens_lists]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_text_classification_pipeline():\n",
    "    pipeline = Pipeline([\n",
    "        # 1. Tokenizar texto\n",
    "        ('tokenizer', TextTokenizer(text_col='Descripcion')),\n",
    "        \n",
    "        # 2. Preprocesar tokens\n",
    "        ('token_preprocessor', TokenPreprocessor()),\n",
    "        \n",
    "        # 3. Lematizar tokens\n",
    "        ('lemmatizer', Lemmatizer()),\n",
    "        \n",
    "        # 4. Unir tokens para vectorización\n",
    "        ('join_tokens', JoinTokens()),\n",
    "        \n",
    "        # 5. Vectorizar texto\n",
    "        ('vectorizer', TfidfVectorizer()),\n",
    "        \n",
    "        # 6. Clasificar\n",
    "        ('classifier', XGBClassifier(eval_metric='mlogloss',\n",
    "                                   n_estimators=100,\n",
    "                                   max_depth=5,\n",
    "                                   learning_rate=0.1,\n",
    "                                   random_state=42))\n",
    "    ])\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(df, text_col='Descripcion', target_col='Label', test_size=0.2, random_state=42):\n",
    "    # Eliminar duplicados y conflictos a nivel de dataframe\n",
    "    print(f\"Filas originales: {len(df)}\")\n",
    "    \n",
    "    # Detectar duplicados \n",
    "    duplicated_descriptions = df[df[text_col].duplicated(keep=False)]\n",
    "    \n",
    "    # Identificar duplicados conflictivos (mismo texto, diferente etiqueta)\n",
    "    conflict_groups = duplicated_descriptions.groupby(text_col)[target_col].nunique() > 1\n",
    "    conflict_texts = conflict_groups[conflict_groups].index\n",
    "    \n",
    "    # Eliminar filas con conflictos\n",
    "    conflict_rows = df[df[text_col].isin(conflict_texts)].index\n",
    "    df_clean = df.drop(index=conflict_rows)\n",
    "    print(f\"Filas eliminadas por conflictos: {len(conflict_rows)}\")\n",
    "    \n",
    "    # Eliminar duplicados restantes\n",
    "    df_clean = df_clean.drop_duplicates(subset=[text_col], keep='first')\n",
    "    print(f\"Filas después de eliminar duplicados y conflictos: {len(df_clean)}\")\n",
    "    \n",
    "    # Dividir en entrenamiento y prueba\n",
    "    df_train, df_test = train_test_split(\n",
    "        df_clean, test_size=test_size, random_state=random_state, stratify=df_clean[target_col]\n",
    "    )\n",
    "    \n",
    "    # Preparar X e y para entrenamiento\n",
    "    X_train = df_train[[text_col]].copy()\n",
    "    y_train = df_train[target_col]\n",
    "    \n",
    "    # Preparar X e y para prueba\n",
    "    X_test = df_test[[text_col]].copy()\n",
    "    y_test = df_test[target_col]\n",
    "    \n",
    "    # Crear y entrenar el pipeline\n",
    "    pipeline = create_text_classification_pipeline()\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluar el modelo\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    # Métricas\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score (weighted): {f1:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "    \n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_model(pipeline, file_path=\"modelo_clasificacion_texto.joblib\"):\n",
    "    joblib.dump(pipeline, file_path)\n",
    "    print(f\"Modelo exportado exitosamente a: {file_path}\")\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/fake_news_spanish.csv\", sep=\";\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Label</th>\n",
       "      <th>Titulo</th>\n",
       "      <th>Descripcion</th>\n",
       "      <th>Fecha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID</td>\n",
       "      <td>1</td>\n",
       "      <td>'The Guardian' va con Sánchez: 'Europa necesit...</td>\n",
       "      <td>El diario británico publicó este pasado jueves...</td>\n",
       "      <td>02/06/2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID</td>\n",
       "      <td>0</td>\n",
       "      <td>REVELAN QUE EL GOBIERNO NEGOCIO LA LIBERACIÓN ...</td>\n",
       "      <td>REVELAN QUE EL GOBIERNO NEGOCIO LA LIBERACIÓN ...</td>\n",
       "      <td>01/10/2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID</td>\n",
       "      <td>1</td>\n",
       "      <td>El 'Ahora o nunca' de Joan Fuster sobre el est...</td>\n",
       "      <td>El valencianismo convoca en Castelló su fiesta...</td>\n",
       "      <td>25/04/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID</td>\n",
       "      <td>1</td>\n",
       "      <td>Iglesias alienta a Yolanda Díaz, ERC y EH Bild...</td>\n",
       "      <td>En política, igual que hay que negociar con lo...</td>\n",
       "      <td>03/01/2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID</td>\n",
       "      <td>0</td>\n",
       "      <td>Puigdemont: 'No sería ninguna tragedia una rep...</td>\n",
       "      <td>En una entrevista en El Punt Avui, el líder de...</td>\n",
       "      <td>09/03/2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Label                                             Titulo  \\\n",
       "0  ID      1  'The Guardian' va con Sánchez: 'Europa necesit...   \n",
       "1  ID      0  REVELAN QUE EL GOBIERNO NEGOCIO LA LIBERACIÓN ...   \n",
       "2  ID      1  El 'Ahora o nunca' de Joan Fuster sobre el est...   \n",
       "3  ID      1  Iglesias alienta a Yolanda Díaz, ERC y EH Bild...   \n",
       "4  ID      0  Puigdemont: 'No sería ninguna tragedia una rep...   \n",
       "\n",
       "                                         Descripcion       Fecha  \n",
       "0  El diario británico publicó este pasado jueves...  02/06/2023  \n",
       "1  REVELAN QUE EL GOBIERNO NEGOCIO LA LIBERACIÓN ...  01/10/2023  \n",
       "2  El valencianismo convoca en Castelló su fiesta...  25/04/2022  \n",
       "3  En política, igual que hay que negociar con lo...  03/01/2022  \n",
       "4  En una entrevista en El Punt Avui, el líder de...  09/03/2018  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas originales: 57063\n",
      "Filas eliminadas por conflictos: 13880\n",
      "Filas después de eliminar duplicados y conflictos: 42787\n",
      "Accuracy: 0.8662\n",
      "F1 Score (weighted): 0.8602\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.68      0.80      3386\n",
      "           1       0.82      0.99      0.90      5172\n",
      "\n",
      "    accuracy                           0.87      8558\n",
      "   macro avg       0.90      0.83      0.85      8558\n",
      "weighted avg       0.88      0.87      0.86      8558\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar el pipeline de entrenamiento y evaluación\n",
    "pipeline = train_and_evaluate(df, text_col='Descripcion', target_col='Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo exportado exitosamente a: modelo_clasificacion_texto.joblib\n"
     ]
    }
   ],
   "source": [
    "model_path = export_model(pipeline, \"modelo_clasificacion_texto.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
